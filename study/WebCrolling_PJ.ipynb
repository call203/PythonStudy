{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오늘 날짜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "day = now.strftime(\"%Y-%m-%d\")\n",
    "print(day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뉴스 리스트 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#스포츠 메뉴의 구조가 달라 따로 구함\n",
    "def sport(Sports_list):\n",
    "    for i in range(3):\n",
    "        params={\n",
    "            'page':i+1,\n",
    "            's_mcd':'0107'\n",
    "        }\n",
    "        resp = requests.get('https://www.ytn.co.kr/photo/photo_list.php',params=params)\n",
    "        soup = BeautifulSoup(resp.text)\n",
    "        #리스트\n",
    "        sec_tag = soup.find(\"div\", id=\"ytn_list_v2014\")\n",
    "        dl_tag=sec_tag.find_all(\"dl\",class_='photo_list')\n",
    "        \n",
    "        for j in dl_tag:\n",
    "            # 날짜\n",
    "            date_tag = j.find('dd',class_='date')\n",
    "            date_d = date_tag.text\n",
    "            date = date_d.split(\" \")\n",
    "            date = date[0].replace(\"[\",\"\")\n",
    "\n",
    "\n",
    "            if day == date:\n",
    "                dt_tag =j.find_all('dt')\n",
    "                a_tag = dt_tag[0].find(\"a\")\n",
    "                title = a_tag.text\n",
    "                \n",
    "                \n",
    "                if title in Sports_list:\n",
    "                    continue\n",
    "                    \n",
    "                Sports_list.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.ytn.co.kr/news/news_list_0101.html'\n",
    "\n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.content)\n",
    "menu = soup.find(\"ul\",id='sub_1')\n",
    "li_tag = menu.find_all('li',recursive=False)\n",
    "\n",
    "a=[]\n",
    "politics_list=[]\n",
    "economy_list=[]\n",
    "social_list=[]\n",
    "domestic_list=[]\n",
    "International_list=[]\n",
    "Science_list=[]\n",
    "Sports_list=[]\n",
    "culture_list =[]\n",
    "newlist = [politics_list,economy_list,social_list,domestic_list,\n",
    "           International_list,Science_list,culture_list]\n",
    "#3,4,5,6,7,8,9,10,11\n",
    "url2 = 'https://www.ytn.co.kr/news/news_list.php?'\n",
    "for idx in range(3,11,1):\n",
    "    #메뉴이동\n",
    "    a_tag =li_tag[idx].find('a')\n",
    "    move = a_tag.get('href')\n",
    "    a = re.findall('\\d+',move)\n",
    "    \n",
    "    if idx == 10:\n",
    "        sport(Sports_list)\n",
    "        break\n",
    "          \n",
    "    for i in range(3):\n",
    "        params={\n",
    "            'page':i+1,\n",
    "            's_mcd':a[0]\n",
    "        }\n",
    "        resp = requests.get(url2,params=params)\n",
    "        soup = BeautifulSoup(resp.text)\n",
    "\n",
    "        #리스트\n",
    "        sec_tag = soup.find(\"div\", id=\"ytn_list_v2014\")\n",
    "        \n",
    "        span_tag=sec_tag.find_all(\"dl\", class_=\"news_list_v2014\")\n",
    "\n",
    "        for j in span_tag:\n",
    "            # 날짜\n",
    "            date_tag = j.find('dd',class_='date')\n",
    "            date_d = date_tag.text\n",
    "            date = date_d.split(\" \")\n",
    "            date = date[0].replace(\"[\",\"\")\n",
    "\n",
    "\n",
    "            if day == date:\n",
    "                dt_tag =j.find_all('dt')\n",
    "                a_tag = dt_tag[0].find(\"a\")\n",
    "                title = a_tag.text\n",
    "                \n",
    "                \n",
    "                if title in newlist[idx-3]:\n",
    "                    continue\n",
    "                    \n",
    "                newlist[idx-3].append(title)\n",
    "           \n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('list/politics.txt','w')\n",
    "\n",
    "for i in politics_list:\n",
    "    msg = i\n",
    "    f.write(i+\"\\n\")\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('list/social.txt','w')\n",
    "\n",
    "for i in social_list:\n",
    "    msg = i\n",
    "    f.write(i+\"\\n\")\n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('list/economy.txt','w')\n",
    "\n",
    "for i in economy_list:\n",
    "    msg = i\n",
    "    f.write(i+\"\\n\")\n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('list/domestic.txt','w')\n",
    "\n",
    "for i in domestic_list:\n",
    "    msg = i\n",
    "    f.write(i+\"\\n\")\n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('list/International.txt','w')\n",
    "\n",
    "for i in International_list:\n",
    "    msg = i\n",
    "    f.write(i+\"\\n\")\n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('list/Science.txt','w')\n",
    "\n",
    "for i in Science_list:\n",
    "    msg = i\n",
    "    f.write(i+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('list/culture.txt','w')\n",
    "\n",
    "for i in culture_list:\n",
    "    msg = i\n",
    "    f.write(i+\"\\n\")\n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('list/Sports.txt','w')\n",
    "\n",
    "for i in Sports_list:\n",
    "    msg = i\n",
    "    f.write(i+\"\\n\")\n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 명사 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_list=['list/politics.txt','list/social.txt','list/economy.txt','list/domestic.txt',\n",
    "          'list/International.txt','list/Science.txt','list/culture.txt','list/Sports.txt']\n",
    "\n",
    "\n",
    "def get_tags(text, ntags=50):\n",
    "    spliter = Twitter()\n",
    "    \n",
    "    # konlpy의 Twitter객체\n",
    "    nouns = spliter.nouns(text)\n",
    "    \n",
    "    # nouns 함수를 통해서 text에서 명사만 분리/추출\n",
    "    count = Counter(nouns)\n",
    "    \n",
    "    \n",
    "    # Counter객체를 생성하고 참조변수 nouns할당\n",
    "    return_list = []  # 명사 빈도수 저장할 변수\n",
    "    for n, c in count.most_common(ntags):\n",
    "        temp = {'tag': n, 'count': c}\n",
    "        return_list.append(temp)\n",
    "        \n",
    "        \n",
    "    # most_common 메소드는 정수를 입력받아 객체 안의 명사중 빈도수\n",
    "    # 큰 명사부터 순서대로 입력받은 정수 갯수만큼 저장되어있는 객체 반환\n",
    "    # 명사와 사용된 갯수를 return_list에 저장합니다.\n",
    "    return return_list\n",
    "\n",
    "def main():\n",
    "    for i in menu_list:\n",
    "        text_file_name = i\n",
    "        # 분석할 파일\n",
    "        \n",
    "        noun_count = 20\n",
    "        # 최대 많은 빈도수 부터 20개 명사 추출\n",
    "        \n",
    "        \n",
    "        output_file_name = i\n",
    "        # count.txt 에 저장\n",
    "        \n",
    "        \n",
    "        open_text_file = open(text_file_name, 'r',encoding='euc-kr')\n",
    "        # 분석할 파일을 open \n",
    "        i = i.split(\"/\")\n",
    "        \n",
    "        text = open_text_file.read() #파일을 읽습니다.\n",
    "        tags = get_tags(text, noun_count) # get_tags 함수 실행\n",
    "        open_text_file.close()   #파일 close\n",
    "        open_output_file = open('list/count/' + i[1], 'w',encoding='euc-kr')\n",
    "        # 결과로 쓰일 count.txt 열기\n",
    "        for tag in tags:\n",
    "            noun = tag['tag']\n",
    "            count = tag['count']\n",
    "            open_output_file.write('{} {}\\n'.format(noun, count))\n",
    "        # 결과 저장\n",
    "        open_output_file.close() \n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('부동산', '9')\n",
      "('본회의', '8')\n",
      "('국회', '8')\n",
      "('통합', '7')\n",
      "('공급', '7')\n",
      "('법', '7')\n",
      "('주택', '6')\n",
      "('법안', '6')\n",
      "('통과', '6')\n",
      "('속보', '6')\n",
      "('오늘', '6')\n",
      "('민주당', '5')\n",
      "('대책', '5')\n",
      "('후속', '5')\n",
      "('대통령', '5')\n",
      "('점검', '5')\n",
      "('정', '4')\n",
      "('집중호우', '4')\n",
      "('것', '3')\n",
      "('처리', '3')\n",
      "('',)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 0; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-bfe693ed8ccb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mfont_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Windows/Fonts/malgun.ttf'\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mwc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfont_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground_color\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'white'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 0; 2 is required"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "from IPython.display import set_matplotlib_formats\n",
    "count_list=['list/count/politics.txt','list/count/social.txt','list/count/economy.txt','list/count/domestic.txt',\n",
    "           'list/count/International.txt','list/count/Science.txt','list/count/culture.txt','list/count/Sports.txt']\n",
    "\n",
    "for i in count_list:\n",
    "    open_text_file = open(i, 'r',encoding='euc-kr')\n",
    "    text = open_text_file.read()\n",
    "    split1 = text.split(\"\\n\")\n",
    "    for i in split1:\n",
    "        split2 = i.split(\" \")\n",
    "        print(tuple(split2))\n",
    "        \n",
    "    \n",
    "\n",
    "    font_path = 'C:/Windows/Fonts/malgun.ttf';\n",
    "    wc = WordCloud(font_path = font_path, background_color = 'white', width=800, height=600)\n",
    "    cloud = wc.generate_from_frequencies(dict(tags))\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.axis('off')\n",
    "    print(i)\n",
    "    plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
