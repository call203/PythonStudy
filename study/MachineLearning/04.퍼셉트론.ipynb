{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "     - 신경망을 이루는 가장 중요한 기본 단위는 퍼셉트론\n",
    "     - 퍼셉트론 : 입력 값과 활성화 함수를 사용해 출력 값을 다음으로 넘기는 가장 작은 신경망 단위\n",
    "     \n",
    "     - 기울기 = 가중치 (w), y절편 = 바이어스 (b)\n",
    "     - 가중합의 결과를 놓고 1또는 0을 출력해서 다음으로 보냄\n",
    "     - 활성화 함수를 통해 1과 0을 판단해 출력함\n",
    "     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Single neural network AND 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  0 cost=  1.057761 W=  [[1.1790476]\n",
      " [2.7920907]] b=  [-0.8022166]\n",
      "step =  100 cost=  0.7744708 W=  [[0.6045825]\n",
      " [1.5046046]] b=  [-1.1998127]\n",
      "step =  200 cost=  0.73048544 W=  [[0.4744897 ]\n",
      " [0.95994884]] b=  [-0.85397816]\n",
      "step =  300 cost=  0.7101486 W=  [[0.36056906]\n",
      " [0.6208953 ]] b=  [-0.5834466]\n",
      "step =  400 cost=  0.70084876 W=  [[0.2638228 ]\n",
      " [0.40309313]] b=  [-0.3959346]\n",
      "step =  500 cost=  0.69663334 W=  [[0.18855013]\n",
      " [0.26299262]] b=  [-0.2679167]\n",
      "step =  600 cost=  0.6947268 W=  [[0.13271523]\n",
      " [0.17249273]] b=  [-0.1810456]\n",
      "step =  700 cost=  0.69386387 W=  [[0.09244291]\n",
      " [0.11369473]] b=  [-0.12226492]\n",
      "step =  800 cost=  0.69347274 W=  [[0.06391151]\n",
      " [0.07526501]] b=  [-0.08254471]\n",
      "step =  900 cost=  0.69329524 W=  [[0.0439432 ]\n",
      " [0.05000852]] b=  [-0.05572098]\n",
      "step =  1000 cost=  0.69321454 W=  [[0.03008886]\n",
      " [0.03332905]] b=  [-0.0376116]\n",
      "step =  1100 cost=  0.6931778 W=  [[0.02053753]\n",
      " [0.02226847]] b=  [-0.02538706]\n",
      "step =  1200 cost=  0.69316113 W=  [[0.01398409]\n",
      " [0.01490878]] b=  [-0.01713555]\n",
      "step =  1300 cost=  0.6931535 W=  [[0.00950389]\n",
      " [0.00999788]] b=  [-0.01156595]\n",
      "step =  1400 cost=  0.69315004 W=  [[0.00644955]\n",
      " [0.00671346]] b=  [-0.00780663]\n",
      "step =  1500 cost=  0.6931485 W=  [[0.00437181]\n",
      " [0.00451279]] b=  [-0.00526921]\n",
      "step =  1600 cost=  0.6931478 W=  [[0.00296073]\n",
      " [0.00303606]] b=  [-0.00355653]\n",
      "step =  1700 cost=  0.6931474 W=  [[0.0020037 ]\n",
      " [0.00204393]] b=  [-0.00240055]\n",
      "step =  1800 cost=  0.6931473 W=  [[0.00135527]\n",
      " [0.00137677]] b=  [-0.00162028]\n",
      "step =  1900 cost=  0.69314724 W=  [[0.0009163 ]\n",
      " [0.00092778]] b=  [-0.00109363]\n",
      "step =  2000 cost=  0.69314724 W=  [[0.00061926]\n",
      " [0.0006254 ]] b=  [-0.00073818]\n",
      "step =  2100 cost=  0.6931472 W=  [[0.00041846]\n",
      " [0.00042174]] b=  [-0.00049823]\n",
      "step =  2200 cost=  0.6931472 W=  [[0.00028266]\n",
      " [0.00028441]] b=  [-0.00033632]\n",
      "step =  2300 cost=  0.6931472 W=  [[0.0001909 ]\n",
      " [0.00019184]] b=  [-0.000227]\n",
      "step =  2400 cost=  0.6931472 W=  [[0.00012892]\n",
      " [0.00012942]] b=  [-0.00015321]\n",
      "step =  2500 cost=  0.6931472 W=  [[8.7046625e-05]\n",
      " [8.7310895e-05]] b=  [-0.0001034]\n",
      "step =  2600 cost=  0.6931472 W=  [[5.876570e-05]\n",
      " [5.890779e-05]] b=  [-6.978424e-05]\n",
      "step =  2700 cost=  0.6931472 W=  [[3.9672828e-05]\n",
      " [3.9746377e-05]] b=  [-4.7094243e-05]\n",
      "step =  2800 cost=  0.6931472 W=  [[2.6778851e-05]\n",
      " [2.6816639e-05]] b=  [-3.178107e-05]\n",
      "step =  2900 cost=  0.6931472 W=  [[1.8069119e-05]\n",
      " [1.8087538e-05]] b=  [-2.145307e-05]\n",
      "step =  3000 cost=  0.69314724 W=  [[1.2195081e-05]\n",
      " [1.2206049e-05]] b=  [-1.4471128e-05]\n",
      "step =  3100 cost=  0.6931472 W=  [[8.220943e-06]\n",
      " [8.227440e-06]] b=  [-9.775778e-06]\n",
      "step =  3200 cost=  0.6931472 W=  [[5.5387363e-06]\n",
      " [5.5407636e-06]] b=  [-6.6107687e-06]\n",
      "step =  3300 cost=  0.6931472 W=  [[3.7461236e-06]\n",
      " [3.7481509e-06]] b=  [-4.435195e-06]\n",
      "step =  3400 cost=  0.6931472 W=  [[2.515290e-06]\n",
      " [2.515827e-06]] b=  [-3.019585e-06]\n",
      "step =  3500 cost=  0.6931472 W=  [[1.7091335e-06]\n",
      " [1.7096705e-06]] b=  [-2.004816e-06]\n",
      "step =  3600 cost=  0.6931472 W=  [[1.1562975e-06]\n",
      " [1.1568345e-06]] b=  [-1.3469303e-06]\n",
      "step =  3700 cost=  0.6931471 W=  [[7.599250e-07]\n",
      " [7.604621e-07]] b=  [-9.3714755e-07]\n",
      "step =  3800 cost=  0.6931472 W=  [[5.0362485e-07]\n",
      " [5.0416196e-07]] b=  [-6.5328004e-07]\n",
      "step =  3900 cost=  0.6931472 W=  [[3.5461315e-07]\n",
      " [3.5515029e-07]] b=  [-4.6477996e-07]\n",
      "step =  4000 cost=  0.6931471 W=  [[2.1305158e-07]\n",
      " [2.1358872e-07]] b=  [-3.0831717e-07]\n",
      "step =  4100 cost=  0.6931472 W=  [[1.6089732e-07]\n",
      " [1.6143446e-07]] b=  [-2.0400864e-07]\n",
      "step =  4200 cost=  0.6931472 W=  [[1.2960476e-07]\n",
      " [1.3014190e-07]] b=  [-1.4142353e-07]\n",
      "step =  4300 cost=  0.6931472 W=  [[1.04272765e-07]\n",
      " [1.04809899e-07]] b=  [-9.224951e-08]\n",
      "step =  4400 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  4500 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  4600 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  4700 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  4800 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  4900 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  5000 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  5100 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  5200 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  5300 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  5400 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  5500 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  5600 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  5700 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  5800 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  5900 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  6000 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  6100 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  6200 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  6300 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  6400 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  6500 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  6600 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  6700 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  6800 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  6900 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  7000 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  7100 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  7200 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  7300 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  7400 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  7500 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  7600 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  7700 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  7800 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  7900 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  8000 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  8100 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  8200 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  8300 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  8400 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  8500 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  8600 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  8700 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  8800 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  8900 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  9000 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  9100 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  9200 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  9300 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  9400 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  9500 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  9600 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  9700 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  9800 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  9900 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "step =  10000 cost=  0.6931472 W=  [[8.788150e-08]\n",
      " [8.841864e-08]] b=  [-5.946683e-08]\n",
      "\n",
      "Hypothesis: [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct(y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      " Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,2],name='x_input')\n",
    "Y = tf.placeholder(tf.float32, [None,1],name='y_input')\n",
    "\n",
    "#가중치\n",
    "W = tf.Variable(tf.random_normal([2,1], name='weight'))\n",
    "#바이어스\n",
    "b = tf.Variable(tf.random_normal([1]),name='bias')\n",
    "\n",
    "#활성화 함수\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "#cost/loss\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "#예측값 결과 계산 및 정밀도 계산\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "\n",
    "\n",
    "#Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train,feed_dict={X:x_data,Y:y_data})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"step = \",step,\"cost= \",sess.run(cost,feed_dict={X:x_data,Y:y_data}), \"W= \", sess.run(W), \"b= \", sess.run(b))\n",
    "            \n",
    "    h,c,a=sess.run([hypothesis,predicted,accuracy], feed_dict ={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis:\",h,\"\\nCorrect(y): \",c,\"\\n Accuracy:\",a)\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다중 퍼셉트론 [ XOR ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```  \n",
    "    1. 따라서 만약 두개의 직선(다중)을 만들기 위해서 두개의 뉴런이 필요[hidden layer]\n",
    "    2. 이 둘을 교차시키는 연산이 필요함. \n",
    "    3. 여기서 활성함수로는 시그모이드를 사용함. \n",
    "    \n",
    "    \n",
    "    * 여기서 AND,OR,NAND를 (hidden layer로 구성되는) 만족하는 가중치와 바이어스의 조합은 무수히 많음.\n",
    "    \n",
    "    y = w1*1 _ w2*2 + b\n",
    "    \n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력값 : (0, 0)출력값:0\n",
      "입력값 : (1, 0)출력값:1\n",
      "입력값 : (0, 1)출력값:1\n",
      "입력값 : (1, 1)출력값:0\n"
     ]
    }
   ],
   "source": [
    "################   NUMPY로 짜보기   ############\n",
    "\n",
    "\n",
    "#가중치와 바이어스\n",
    "w11 = np.array([-7.40,-7.40])\n",
    "w12 = np.array([8.67,8.67])\n",
    "w2 = np.array([7.41,7.41])\n",
    "b1 = 11.28\n",
    "b2 = -2.87\n",
    "b3 = -11.29\n",
    "\n",
    "#퍼센트론\n",
    "def MLP(x ,w ,b):\n",
    "    y = np.sum(w*x) + b\n",
    "    if y <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "#NAND 게이트\n",
    "def NAND(x1, x2):\n",
    "    return MLP(np.array([x1,x2]),w11,b1)\n",
    "\n",
    "def OR(x1, x2):\n",
    "    return MLP(np.array([x1, x2]), w12, b2)\n",
    "\n",
    "#AND 게이트\n",
    "def AND(x1, x2):\n",
    "    return MLP(np.array([x1, x2]), w2,b3)\n",
    "\n",
    "def XOR(x1, x2):\n",
    "    return AND(NAND(x1, x2), OR(x1,x2))\n",
    "\n",
    "\n",
    "#x1, x2 값을 번갈아 대입해 가며 최종값 출력\n",
    "if __name__ == '__main__':\n",
    "    for x in [(0,0),(1,0),(0,1),(1,1)]:\n",
    "        y = XOR(x[0],x[1])\n",
    "        print(\"입력값 : \"+ str(x) + \"출력값:\" + str(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오차 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    신경망 내부의 가중치는 오차 역전파 방법을 사용해 수정\n",
    "    * 가중치  --> 경사 하강법 사용!\n",
    "                  실제값(y)이 나오면 출력층에서 시작해 뒤에서 앞(hidden layer 쪽)으로 진행됨 \n",
    "                  이과정에서 가중치에서 기울기를 뺏을 때 가중치 변화가 없을 때까지 나아감.\n",
    "                  --> 오차 역전파\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1549672 [[-0.4460009]\n",
      " [-1.5890768]]\n",
      "100 1.1343694 [[-0.49363706]\n",
      " [-1.6886524 ]]\n",
      "200 1.1330063 [[-0.47585425]\n",
      " [-1.6574812 ]]\n",
      "300 1.1318729 [[-0.45962474]\n",
      " [-1.6305039 ]]\n",
      "400 1.1309117 [[-0.4446806]\n",
      " [-1.6070787]]\n",
      "500 1.1300824 [[-0.43080032]\n",
      " [-1.586696  ]]\n",
      "600 1.1293547 [[-0.4178094]\n",
      " [-1.5689653]]\n",
      "700 1.1287065 [[-0.40556934]\n",
      " [-1.5535817 ]]\n",
      "800 1.1281204 [[-0.39396918]\n",
      " [-1.5403051 ]]\n",
      "900 1.1275835 [[-0.3829196]\n",
      " [-1.5289443]]\n",
      "1000 1.127085 [[-0.37234983]\n",
      " [-1.5193428 ]]\n",
      "1100 1.1266166 [[-0.36220458]\n",
      " [-1.5113732 ]]\n",
      "1200 1.1261712 [[-0.3524423]\n",
      " [-1.5049274]]\n",
      "1300 1.1257434 [[-0.34303394]\n",
      " [-1.4999119 ]]\n",
      "1400 1.1253285 [[-0.3339632]\n",
      " [-1.4962431]]\n",
      "1500 1.124923 [[-0.3252257]\n",
      " [-1.4938422]]\n",
      "1600 1.1245238 [[-0.31682986]\n",
      " [-1.4926308 ]]\n",
      "1700 1.1241288 [[-0.3087968]\n",
      " [-1.4925282]]\n",
      "1800 1.1237363 [[-0.3011608]\n",
      " [-1.4934486]]\n",
      "1900 1.1233456 [[-0.29397053]\n",
      " [-1.495296  ]]\n",
      "2000 1.1229562 [[-0.28728884]\n",
      " [-1.4979644 ]]\n",
      "2100 1.1225684 [[-0.2811928]\n",
      " [-1.501335 ]]\n",
      "2200 1.1221824 [[-0.2757742]\n",
      " [-1.5052781]]\n",
      "2300 1.121799 [[-0.2711373]\n",
      " [-1.5096544]]\n",
      "2400 1.1214192 [[-0.2673988]\n",
      " [-1.5143186]]\n",
      "2500 1.1210432 [[-0.26468486]\n",
      " [-1.5191246 ]]\n",
      "2600 1.1206713 [[-0.26313072]\n",
      " [-1.5239304 ]]\n",
      "2700 1.1203034 [[-0.26287672]\n",
      " [-1.528607  ]]\n",
      "2800 1.1199385 [[-0.26406696]\n",
      " [-1.5330426 ]]\n",
      "2900 1.1195751 [[-0.26684797]\n",
      " [-1.537147  ]]\n",
      "3000 1.1192117 [[-0.27136678]\n",
      " [-1.5408548 ]]\n",
      "3100 1.1188452 [[-0.27776992]\n",
      " [-1.5441275 ]]\n",
      "3200 1.1184732 [[-0.286204 ]\n",
      " [-1.5469499]]\n",
      "3300 1.1180921 [[-0.29681504]\n",
      " [-1.5493307 ]]\n",
      "3400 1.1176984 [[-0.30974835]\n",
      " [-1.5512984 ]]\n",
      "3500 1.1172879 [[-0.3251474]\n",
      " [-1.5528992]]\n",
      "3600 1.1168561 [[-0.34315386]\n",
      " [-1.5541939 ]]\n",
      "3700 1.116398 [[-0.36390448]\n",
      " [-1.5552574 ]]\n",
      "3800 1.1159079 [[-0.3875293]\n",
      " [-1.5561767]]\n",
      "3900 1.11538 [[-0.414146 ]\n",
      " [-1.5570525]]\n",
      "4000 1.1148074 [[-0.44385567]\n",
      " [-1.5579994 ]]\n",
      "4100 1.114183 [[-0.47673672]\n",
      " [-1.5591465 ]]\n",
      "4200 1.1134992 [[-0.512838 ]\n",
      " [-1.5606394]]\n",
      "4300 1.1127477 [[-0.552172 ]\n",
      " [-1.5626415]]\n",
      "4400 1.1119201 [[-0.5947113]\n",
      " [-1.5653332]]\n",
      "4500 1.1110079 [[-0.6403835]\n",
      " [-1.5689118]]\n",
      "4600 1.1100026 [[-0.6890724]\n",
      " [-1.5735875]]\n",
      "4700 1.1088951 [[-0.7406203]\n",
      " [-1.5795827]]\n",
      "4800 1.107676 [[-0.7948366]\n",
      " [-1.5871228]]\n",
      "4900 1.106336 [[-0.8515071]\n",
      " [-1.5964315]]\n",
      "5000 1.1048645 [[-0.91040516]\n",
      " [-1.6077257 ]]\n",
      "5100 1.1032504 [[-0.9713061]\n",
      " [-1.6212075]]\n",
      "5200 1.1014814 [[-1.0339966]\n",
      " [-1.637062 ]]\n",
      "5300 1.0995443 [[-1.0982856]\n",
      " [-1.6554532]]\n",
      "5400 1.0974241 [[-1.1640114]\n",
      " [-1.6765234]]\n",
      "5500 1.0951052 [[-1.2310483]\n",
      " [-1.7003957]]\n",
      "5600 1.0925701 [[-1.2993127]\n",
      " [-1.7271701]]\n",
      "5700 1.0898013 [[-1.36877  ]\n",
      " [-1.7569239]]\n",
      "5800 1.0867815 [[-1.439433 ]\n",
      " [-1.7897072]]\n",
      "5900 1.0834948 [[-1.511372 ]\n",
      " [-1.8255397]]\n",
      "6000 1.0799298 [[-1.5846996]\n",
      " [-1.8644075]]\n",
      "6100 1.076081 [[-1.6595545]\n",
      " [-1.906263 ]]\n",
      "6200 1.0719533 [[-1.7360708]\n",
      " [-1.9510274]]\n",
      "6300 1.0675629 [[-1.8143381]\n",
      " [-1.998596 ]]\n",
      "6400 1.0629406 [[-1.894363 ]\n",
      " [-2.0488431]]\n",
      "6500 1.05813 [[-1.9760365]\n",
      " [-2.1016226]]\n",
      "6600 1.0531867 [[-2.0591214]\n",
      " [-2.1567671]]\n",
      "6700 1.0481735 [[-2.1432626]\n",
      " [-2.2140815]]\n",
      "6800 1.0431561 [[-2.2280145]\n",
      " [-2.2733371]]\n",
      "6900 1.0381979 [[-2.3128805]\n",
      " [-2.3342676]]\n",
      "7000 1.0333562 [[-2.3973598]\n",
      " [-2.3965683]]\n",
      "7100 1.0286789 [[-2.480979 ]\n",
      " [-2.4599116]]\n",
      "7200 1.0242038 [[-2.5633192]\n",
      " [-2.5239515]]\n",
      "7300 1.0199566 [[-2.6440327]\n",
      " [-2.5883431]]\n",
      "7400 1.015954 [[-2.7228465]\n",
      " [-2.6527557]]\n",
      "7500 1.0122032 [[-2.7995572]\n",
      " [-2.7168837]]\n",
      "7600 1.0087047 [[-2.874031 ]\n",
      " [-2.7804577]]\n",
      "7700 1.0054529 [[-2.9461854]\n",
      " [-2.8432438]]\n",
      "7800 1.0024385 [[-3.0159864]\n",
      " [-2.9050467]]\n",
      "7900 0.9996499 [[-3.0834332]\n",
      " [-2.9657106]]\n",
      "8000 0.9970734 [[-3.1485565]\n",
      " [-3.0251172]]\n",
      "8100 0.99469465 [[-3.211402 ]\n",
      " [-3.0831769]]\n",
      "8200 0.9924991 [[-3.2720354]\n",
      " [-3.1398304]]\n",
      "8300 0.99047244 [[-3.3305314]\n",
      " [-3.195041 ]]\n",
      "8400 0.9886011 [[-3.3869696]\n",
      " [-3.24879  ]]\n",
      "8500 0.9868721 [[-3.4414346]\n",
      " [-3.301076 ]]\n",
      "8600 0.9852731 [[-3.4940104]\n",
      " [-3.3519094]]\n",
      "8700 0.98379326 [[-3.5447826]\n",
      " [-3.4013126]]\n",
      "8800 0.9824219 [[-3.5938346]\n",
      " [-3.4493134]]\n",
      "8900 0.9811497 [[-3.6412482]\n",
      " [-3.4959457]]\n",
      "9000 0.979968 [[-3.687101]\n",
      " [-3.541248]]\n",
      "9100 0.97886896 [[-3.7314692]\n",
      " [-3.5852609]]\n",
      "9200 0.97784543 [[-3.7744246]\n",
      " [-3.6280272]]\n",
      "9300 0.9768909 [[-3.8160362]\n",
      " [-3.6695898]]\n",
      "9400 0.9759997 [[-3.8563688]\n",
      " [-3.7099936]]\n",
      "9500 0.9751663 [[-3.8954837]\n",
      " [-3.749281 ]]\n",
      "9600 0.97438616 [[-3.93344  ]\n",
      " [-3.7874944]]\n",
      "9700 0.9736546 [[-3.970293 ]\n",
      " [-3.8246765]]\n",
      "9800 0.97296804 [[-4.006095 ]\n",
      " [-3.8608675]]\n",
      "9900 0.9723226 [[-4.040895]\n",
      " [-3.896107]]\n",
      "10000 0.97171533 [[-4.0747385]\n",
      " [-3.9304318]]\n",
      "\n",
      "Hypothesis:\n",
      " [[0.03501454]\n",
      " [0.3210729 ]\n",
      " [0.31953585]\n",
      " [0.02936292]] \n",
      "Correct:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:\n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "x_data = [[0,0],[0,1],[1,0],[1,1]]\n",
    "y_data=[[0],[1],[1],[0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,2])\n",
    "Y = tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2,2], name ='weight1'))\n",
    "b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2,1]),name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]),name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,W2) + b2)\n",
    "\n",
    "cost =-tf.reduce_mean(Y*tf.log(hypothesis) + (1+Y) *tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train,feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data,Y:y_data}), sess.run(W2))\n",
    "            \n",
    "    h,c,a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis:\\n\",h, \"\\nCorrect:\\n\",c ,\"\\nAccuracy:\\n\",a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
